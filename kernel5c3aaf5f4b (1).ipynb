{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re #import the regular expression library\nimport string\nimport os\nimport json\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.datasets import load_files\nfrom nltk import pos_tag\nfrom nltk.tokenize import RegexpTokenizer\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords #import the stopwords from the ntlk.corpus library\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize #import the word_tokenize method, which is used to turn sentences into words\"\nnltk.download('punkt')\nfrom collections import Counter\nfrom textblob import TextBlob, Word\n\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n\nimport numpy as np\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n!pip install scispacy scipy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\nimport en_core_sci_lg  ","execution_count":1,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nCollecting scispacy\n  Downloading scispacy-0.2.4.tar.gz (38 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (1.4.1)\nRequirement already satisfied: spacy>=2.2.1 in /opt/conda/lib/python3.6/site-packages (from scispacy) (2.2.3)\nCollecting awscli\n  Downloading awscli-1.18.48-py2.py3-none-any.whl (3.0 MB)\n\u001b[K     |████████████████████████████████| 3.0 MB 5.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: conllu in /opt/conda/lib/python3.6/site-packages (from scispacy) (1.3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from scispacy) (1.18.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from scispacy) (0.14.1)\nCollecting nmslib>=1.7.3.6\n  Downloading nmslib-2.0.6-cp36-cp36m-manylinux2010_x86_64.whl (12.9 MB)\n\u001b[K     |████████████████████████████████| 12.9 MB 29.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.3 in /opt/conda/lib/python3.6/site-packages (from scispacy) (0.22.2.post1)\nCollecting pysbd\n  Downloading pysbd-0.2.3-py3-none-any.whl (24 kB)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (1.0.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (46.1.3.post20200330)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (0.9.6)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (2.22.0)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (0.4.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (2.0.3)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (1.0.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (3.0.2)\nRequirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (7.3.1)\nRequirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (1.0.2)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->scispacy) (0.6.0)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (0.3.3)\nCollecting botocore==1.15.48\n  Downloading botocore-1.15.48-py2.py3-none-any.whl (6.1 MB)\n\u001b[K     |████████████████████████████████| 6.1 MB 31.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: colorama<0.4.4,>=0.2.5; python_version != \"3.4\" in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (0.4.3)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (0.15.2)\nCollecting rsa<=3.5.0,>=3.1.2\n  Downloading rsa-3.4.2-py2.py3-none-any.whl (46 kB)\n\u001b[K     |████████████████████████████████| 46 kB 4.6 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: PyYAML<5.4,>=3.10; python_version != \"3.4\" in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (5.3.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from nmslib>=1.7.3.6->scispacy) (5.7.0)\nRequirement already satisfied: pybind11>=2.2.3 in /opt/conda/lib/python3.6/site-packages (from nmslib>=1.7.3.6->scispacy) (2.4.3)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (1.24.3)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (2019.11.28)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (2.8)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->scispacy) (1.5.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.1->scispacy) (4.42.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore==1.15.48->awscli->scispacy) (2.8.1)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from botocore==1.15.48->awscli->scispacy) (0.9.5)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli->scispacy) (0.4.8)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->scispacy) (2.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.15.48->awscli->scispacy) (1.14.0)\nBuilding wheels for collected packages: scispacy\n  Building wheel for scispacy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for scispacy: filename=scispacy-0.2.4-py3-none-any.whl size=35203 sha256=5f139bc4f29e196e85f83c669bede77c95690033ca8e1ff4035a4b99597b3273\n  Stored in directory: /root/.cache/pip/wheels/80/01/69/37a2ab4f9b61773c187d83257ffb31365a5ad57e7779ae5e92\nSuccessfully built scispacy\n\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\nInstalling collected packages: botocore, rsa, awscli, nmslib, pysbd, scispacy\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.15.32\n    Uninstalling botocore-1.15.32:\n      Successfully uninstalled botocore-1.15.32\n  Attempting uninstall: rsa\n    Found existing installation: rsa 4.0\n    Uninstalling rsa-4.0:\n      Successfully uninstalled rsa-4.0\nSuccessfully installed awscli-1.18.48 botocore-1.15.48 nmslib-2.0.6 pysbd-0.2.3 rsa-3.4.2 scispacy-0.2.4\nCollecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz (500.6 MB)\n\u001b[K     |████████████████████████████████| 500.6 MB 11 kB/s  eta 0:00:015   |█▍                              | 21.7 MB 1.0 MB/s eta 0:07:52     |█▌                              | 24.3 MB 1.0 MB/s eta 0:07:49     |█▊                              | 26.6 MB 1.0 MB/s eta 0:07:47     |██                              | 29.9 MB 28.0 MB/s eta 0:00:17     |██▌                             | 39.6 MB 28.0 MB/s eta 0:00:17     |██▉                             | 44.4 MB 28.0 MB/s eta 0:00:17     |█████████▊                      | 152.7 MB 7.3 MB/s eta 0:00:48     |██████████▏                     | 158.4 MB 7.3 MB/s eta 0:00:47     |██████████▋                     | 166.5 MB 63.5 MB/s eta 0:00:06     |████████████████████████████▉   | 450.6 MB 8.0 MB/s eta 0:00:07\n\u001b[?25hRequirement already satisfied: spacy>=2.2.1 in /opt/conda/lib/python3.6/site-packages (from en-core-sci-lg==0.2.4) (2.2.3)\nRequirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (46.1.3.post20200330)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.22.0)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (0.6.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.0.2)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.18.2)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (0.9.6)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (0.4.1)\nRequirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (7.3.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.0.3)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.2)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2019.11.28)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.0.4)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.5.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (4.42.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.2.0)\nBuilding wheels for collected packages: en-core-sci-lg\n  Building wheel for en-core-sci-lg (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for en-core-sci-lg: filename=en_core_sci_lg-0.2.4-py3-none-any.whl size=501343161 sha256=14a5287dd6b78ba6d0528f57db07ac4bab80f1e6be3d43d0e3afa1a944a435d8\n  Stored in directory: /root/.cache/pip/wheels/43/12/d5/dd85b5deab7738797c4d5358672df3616dda39acf570a3ef96\nSuccessfully built en-core-sci-lg\nInstalling collected packages: en-core-sci-lg\nSuccessfully installed en-core-sci-lg-0.2.4\n","name":"stdout"}]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"FullPaper=[]\ndirectories = [\"biorxiv_medrxiv\"]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n        if(dirname=='/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json' ):\n            file_path=f\"{dirname}/{filename}\"\n            paper = json.load(open(file_path,\"rb\"))\n            title = paper['metadata']['title'] \n            try:\n                abstract = paper['abstract']\n            except:\n                abstarct=\"\"                \n            full_text=\"\"     \n\n            for text in paper['body_text']:\n                full_text +=text['text'] +'\\n\\n' \n            FullPaper.append([title,abstract,full_text])\n\nFullPaperDataframe=pd.DataFrame(FullPaper,columns=['title','abstract','full_text'])","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Our aim is to address these points:\n* Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents."},{"metadata":{"trusted":true},"cell_type":"code","source":"full_text = FullPaperDataframe['full_text'].head(1)\n\nclean_text = []\n\n# Init the Wordnet Lemmatizer\nlemmatizer = WordNetLemmatizer()\n\nparser=en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\nall_stopwords = stopwords.words('english')\nour_stopwords=['the','we','it','they','copyright','https','et','al','preprint',\n               'this','these','also','however','although','among','in','medrxiv'\n                'biorxiv','license','without','fig','figure','doi']\nall_stopwords.extend(our_stopwords)\n\npattern = \"\"\"!\"#$%&'()*+,.:;<=>?@[\\]^`{|}~\"\"\"\n\nfor val in tqdm(full_text):\n    body_tokens = word_tokenize(val.lower())\n    paper_body_without_stopwords =  [token for token in body_tokens if not token in all_stopwords]\n    clean_string = ' '.join(paper_body_without_stopwords).lower() #convert the list into string\n    clean_string = re.sub(rf\"[{pattern}]\", '', clean_string)\n    clean_string = re.sub(r'\\b[a-zA-Z]\\b', '', clean_string)\n    clean_string = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', clean_string) \n    clean_text.append(clean_string)\n    \nstemmed_text=[] # a list which will contain the stemmed bodies of all docs    \n\nfor val in tqdm(clean_text): #for the body (which was stripped out of stop words) of each paper\n    stemmed_string=[]\n    words=val.split(' ')\n    for word in words:\n        if(len(word)>1):\n            token = parser(word)\n            lemm_token = lemmatizer.lemmatize(token.text)\n            stemmed_string.append(lemm_token)\n    if(len(stemmed_string)>0):\n        stemmed_text.append(stemmed_string)\n        \nfor i in range(len(stemmed_text)):\n    stemmed_text[i]=\" \".join(stemmed_text[i])\n    \nprint(stemmed_text)","execution_count":12,"outputs":[{"output_type":"stream","text":"100%|██████████| 1/1 [00:00<00:00, 35.67it/s]\n100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n","name":"stderr"},{"output_type":"error","ename":"TypeError","evalue":"can only join an iterable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-f71a9977a71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mstemmed_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mstemmed_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can only join an iterable"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this cell is no longer needed and is kept for reference\n\n\ntext = ' '.join(stemmed_text) #convert all the stemmed bodies into one string containing all the stemmed bodies of all papers \ns=text.split(' ')#split the combination of papers body on space\nvocab_count = Counter(s)# count the number of vocab (unique words)\nmost_occur = vocab_count.most_common(200) #get the most common 200 words\n#print(most_occur) #print the most common 200 words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next, the search engine will be implemented with the help of the tf-idf matrix created above and the cosine similarity\nquery = input(\"Enter your query: \") \nfinalQuery=\"\" #create an empty string\nwords = query[0].split(' ') #split the query on spaces\nprint (words)\nfor word in words: #for each word in the query\n    if(word!=\"\"): #if the word is not an empty string\n        if (word==\"coronaviruses\"):#since the stem and lemmatize functions do not recognize the word coronaviruses as the plural of coronavirus, a conidition was created\n            word =\"coronavirus\"\n        stem_lem_query =wnl.lemmatize(word) if wnl.lemmatize(word).endswith('e') else porter.stem(word)\n        finalQuery+=stem_lem_query+\" \" #stem/lemmatize the word and concat it to the string stemmed_val\nquery = [finalQuery]\nprint (query)\nquery_tfidf = v.transform(query)\ncosineSimilarities = cosine_similarity(query_tfidf, tfidf).flatten()\nprint(cosineSimilarities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a tf-idf matrix for the words above\nv = TfidfVectorizer(sublinear_tf = True, min_df = 0.05,max_df = 0.8)\n#For the above vectorizer, the first paramater specifies that the 1+log(tf) is going to be used instead of tf.\n#The second parameter specifies that if a term appears in less than 5% of the docs, ignore it\n#The third parameter, considers word that do not occur in more than 80% of the corpus\ntfidf = v.fit_transform(stemmed_text) #fit and transform the stemmed texts which were previously cleaned.\nprint(sorted(v.vocabulary_.items(), key=lambda x : x[1]))\nprint(tfidf.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# result of first common 50 words in all the 1943 docs\n#[('preprint', 42993), ('q', 29585), ('license', 20372), ('1', 18167), ('holder', 18100), ('medrxiv', 17564), ('author/funder', 17404), ('cases', 17021), ('peer-reviewed', 16697), ('https', 16223), ('data', 16221), ('covid-19', 15705), ('2', 14649), ('patients', 14258), ('doi', 14057), ('number', 13892), ('available', 13229), ('using', 11934), ('model', 11910), ('figure', 11428), ('display', 11002), ('granted', 10804), ('40', 10803), ('perpetuity', 10751), ('made', 10742), ('also', 10542), ('international', 10212), ('time', 9852), ('fig', 9747), ('3', 9574), ('used', 9344), ('cells', 9104), ('infection', 8833), ('study', 8712), ('sars-cov-2', 8633), ('r', 8195), ('5', 8153), ('this', 8143), ('virus', 8097), ('two', 7845), ('disease', 7820), ('may', 7774), ('s', 7772), ('rate', 7707), ('all', 7528), ('without', 7464), ('biorxiv', 7352), ('one', 7332), ('infected', 7203), ('10', 7201)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf- idf\n# # finding the tf-idf matrix for all the abstracts\n# # vec = TfidfVectorizer()\n\n# # ve = vec.fit_transform(clean_abs)\n\n# # # displaying the tf-idf of the word in the abstact  \n# # pd.DataFrame(ve.toarray(), columns=sorted(vec.vocabulary_.keys()))\n\n# # #applying tf-idf cosine similarity and printing the result(testing)\n# # query = [\"prophylaxis\"]\n# # query_tfidf = vec.transform(query)\n# # cosineSimilarities = cosine_similarity(query_tfidf, ve).flatten()\n# # print(cosineSimilarities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taged_abs=[]\n# nouns=[]\n# # tagging the clean data\n# for val in clean_abs:\n#     taged_abs.append(pos_tag(val.split()))\n# # extracting the nouns\n# for i in range(len(taged_abs)):\n#     doc=[]\n#     for j in range(len(taged_abs[i])):\n#         if(taged_abs[i][j][1]=='NN' or taged_abs[i][j][1]=='NNS'):\n#             doc.append(taged_abs[i][j][0])\n#     nouns.append(doc)\n        \n# print(nouns)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we dcided to calculate the tf-idf so that we can represent every word that is present in the abstract quatitavily. By doing so we can further use the results in order model the topic according to the abstract that we just quatified.Furthermore we will use (Non-negative Matrix Factorization) NMF in order to come up with topic's that carry most weight in the abstract. To accomplish this we are going to filter all the nouns that are avaliable in the abstract and use them to represent the different topis that are avaliable."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}