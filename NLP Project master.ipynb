{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import nltk\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re #import the regular expression library\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import load_files\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords #import the stopwords from the ntlk.corpus library\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize #import the word_tokenize method, which is used to turn sentences into words\"\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1934/1934 [01:12<00:00, 26.51it/s]\n"
     ]
    }
   ],
   "source": [
    "FullPaper=[]\n",
    "directories = [\"biorxiv_medrxiv\"]\n",
    "for directory in directories: #for each of the three folders carrying the json format of different research papers\n",
    "    for file in tqdm(os.listdir(f\"{directory}/{directory}/pdf_json\")): #for every json file\n",
    "        file_path=f\"{directory}/{directory}/pdf_json/{file}\" #set the file path to the file_path variable \n",
    "        paper = json.load(open(file_path,\"rb\"))\n",
    "        title = paper['metadata']['title'] \n",
    "        try:\n",
    "            abstract = paper['abstract']\n",
    "        except:\n",
    "            abstarct=\"\"                \n",
    "        full_text=\"\"     \n",
    "        \n",
    "        for text in paper['body_text']:\n",
    "            full_text +=text['text'] +'\\n\\n' \n",
    "        FullPaper.append([title,abstract,full_text])\n",
    "        \n",
    "FullPaperDataframe=pd.DataFrame(FullPaper,columns=['title','abstract','full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our aim is to address these points:\n",
    "* Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:21<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('preprint', 721), ('data', 395), ('model', 316), ('holder', 313), ('fig', 313), ('author/funder', 307), ('number', 305), ('license', 290), ('peer-reviewed', 288), ('https', 288), ('s', 277), ('medrxiv', 264), ('//doiorg//', 255), ('doi', 255), ('rna', 221), ('also', 221), ('covid-', 221), ('using', 218), ('cases', 216), ('epidemic', 215), ('available', 213), ('used', 196), ('virus', 193), ('rate', 188), ('one', 187), ('a', 184), ('two', 165), ('without', 162), ('this', 162), ('display', 159), ('made', 156), ('time', 156), ('international', 155), ('granted', 155), ('all', 154), ('perpetuity', 154), ('c', 150), ('may', 148), ('different', 147), ('figure', 144), ('biorxiv', 144), ('cells', 144), ('table', 143), ('countries', 142), ('analysis', 140), ('total', 138), ('reads', 137), ('-', 134), ('n', 134), ('would', 133)]\n"
     ]
    }
   ],
   "source": [
    "full_text = FullPaperDataframe['full_text'].head(30)\n",
    "clean_text = [] #a list which will hold all the bodies of the papers after being stripped out of stopwords\n",
    "ps=PorterStemmer() \n",
    "# punctuation_regex = string.punctuation\n",
    "# punctuation_regex = punctuation_regex.replace(\"-\", \"\") # keep the hyphens\n",
    "# pattern = r\"[{}]\".format(punctuation_regex) # generate the regex pattern\n",
    "pattern = \"\"\"!\"#$%&'()*+,.:;<=>?@[\\]^_`{|}~\"\"\" #the pattern which will account for punctuation, and will later be used to remove them\n",
    "#cleaning the data and removing stop words\n",
    "for val in  tqdm(full_text):\n",
    "        body_tokens = word_tokenize(val)\n",
    "        paper_body_without_stopwords = [token for token in body_tokens if not token in stopwords.words('english')] #remove the stop words in the body and return a list\n",
    "        clean_string = ' '.join(paper_body_without_stopwords).lower() #convert the list into string\n",
    "        clean_string = re.sub(rf\"[{pattern}]\", '', clean_string)#remove punctuation except for hyphens\n",
    "        clean_string = re.sub(r\"\\bthe\\b\", r\"\", clean_string) #remove the\n",
    "        clean_string = re.sub(r'\\bwe\\b', '',clean_string ) #remove the pronoun we\n",
    "        clean_string = re.sub(r'\\bit\\b', '',clean_string )#remove the pronoun it\n",
    "        clean_string = re.sub(r'\\bthey\\b', '',clean_string )#remove the pronoun they\n",
    "        clean_string = re.sub(r'\\bcopyright\\b', '',clean_string )#remove the word copyright\n",
    "        clean_string = re.sub(r'\\bet\\b', '',clean_string )#remove the word et\n",
    "        clean_string = re.sub(r'\\bal\\b', '',clean_string )#remove the word al\n",
    "        clean_string = re.sub(r'\\bin\\b', '',clean_string )#remove the preposition in\n",
    "        clean_string = re.sub(r'\\d+', '',clean_string ) #remove digits\n",
    "        clean_text.append(clean_string) #add the string to the list \n",
    "stemmed_text=[] # a list which will contain the stemmed bodies of all docs      \n",
    "for val in clean_text: #for the body (which was stripped out of stop words) of each paper\n",
    "    stemned_val=\"\" #create an empty string\n",
    "    for word in val: #for each word in the body\n",
    "        stemned_val+=ps.stem(word) #stem theword and concat it to the string stemned_val\n",
    "    stemmed_text.append(stemned_val) #add the string stemned_val to the list which will contain the stemmed bodies of all docs  \n",
    "text = ' '.join(stemmed_text) #convert all the stemmed bodies into one string containing all the stemmed bodies of all papers \n",
    "s=text.split(' ')#split on empty string\n",
    "s_without_empty_strings = [string for string in s if string != \"\"]#a list containing all the stemmed words excluding the empty strings\n",
    "r = Counter(s_without_empty_strings)# count the number of vocab (unique words)\n",
    "most_occur = r.most_common(50) #get the most common 50 words\n",
    "print(most_occur) #print the most common 50 words\n",
    "\n",
    "# # finding the tf-idf matrix for all the abstracts\n",
    "# # vec = TfidfVectorizer()\n",
    "# # ve = vec.fit_transform(clean_abs)\n",
    "\n",
    "# # # displaying the tf-idf of the word in the abstact  \n",
    "# # pd.DataFrame(ve.toarray(), columns=sorted(vec.vocabulary_.keys()))\n",
    "\n",
    "# # #applying tf-idf cosine similarity and printing the result(testing)\n",
    "# # query = [\"prophylaxis\"]\n",
    "# # query_tfidf = vec.transform(query)\n",
    "# # cosineSimilarities = cosine_similarity(query_tfidf, ve).flatten()\n",
    "# # print(cosineSimilarities)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taged_abs=[]\n",
    "nouns=[]\n",
    "# tagging the clean data\n",
    "for val in clean_abs:\n",
    "    taged_abs.append(pos_tag(val.split()))\n",
    "# extracting the nouns\n",
    "for i in range(len(taged_abs)):\n",
    "    doc=[]\n",
    "    for j in range(len(taged_abs[i])):\n",
    "        if(taged_abs[i][j][1]=='NN' or taged_abs[i][j][1]=='NNS'):\n",
    "            doc.append(taged_abs[i][j][0])\n",
    "    nouns.append(doc)\n",
    "        \n",
    "print(nouns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we dcided to calculate the tf-idf so that we can represent every word that is present in the abstract quatitavily. By doing so we can further use the results in order model the topic according to the abstract that we just quatified.Furthermore we will use (Non-negative Matrix Factorization) NMF in order to come up with topic's that carry most weight in the abstract. To accomplish this we are going to filter all the nouns that are avaliable in the abstract and use them to represent the different topis that are avaliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
