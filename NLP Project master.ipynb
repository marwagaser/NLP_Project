{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re #import the regular expression library\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import load_files\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords #import the stopwords from the ntlk.corpus library\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize #import the word_tokenize method, which is used to turn sentences into words\"\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1934/1934 [00:04<00:00, 416.75it/s]\n"
     ]
    }
   ],
   "source": [
    "FullPaper=[]\n",
    "directories = [\"biorxiv_medrxiv\"]\n",
    "for directory in directories: #for each of the three folders carrying the json format of different research papers\n",
    "    for file in tqdm(os.listdir(f\"{directory}/{directory}/pdf_json\")): #for every json file\n",
    "        file_path=f\"{directory}/{directory}/pdf_json/{file}\" #set the file path to the file_path variable \n",
    "        paper = json.load(open(file_path,\"rb\"))\n",
    "        title = paper['metadata']['title'] \n",
    "        try:\n",
    "            abstract = paper['abstract']\n",
    "        except:\n",
    "            abstarct=\"\"                \n",
    "        full_text=\"\"     \n",
    "        \n",
    "        for text in paper['body_text']:\n",
    "            full_text +=text['text'] +'\\n\\n' \n",
    "        FullPaper.append([title,abstract,full_text])\n",
    "        \n",
    "FullPaperDataframe=pd.DataFrame(FullPaper,columns=['title','abstract','full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our aim is to address these points:\n",
    "* Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:12<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "full_text = FullPaperDataframe['full_text']\n",
    "clean_text = [] #a list which will hold all the bodies of the papers after being stripped out of stopwords\n",
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "# punctuation_regex = string.punctuation\n",
    "# punctuation_regex = punctuation_regex.replace(\"-\", \"\") # keep the hyphens\n",
    "# pattern = r\"[{}]\".format(punctuation_regex) # generate the regex pattern\n",
    "pattern = \"\"\"!\"#$%&'()*+,.:;<=>?@[\\]^`{|}~\"\"\" #the pattern which will account for punctuation, and will later be used to remove them\n",
    "#cleaning the data and removing stop words\n",
    "for val in  tqdm(full_text):\n",
    "        body_tokens = word_tokenize(val)\n",
    "        paper_body_without_stopwords = [token for token in body_tokens if not token in stopwords.words('english')] #remove the stop words in the body and return a list\n",
    "        clean_string = ' '.join(paper_body_without_stopwords).lower() #convert the list into string\n",
    "        #clean_string = re.sub(rf\"[{pattern}]\", '', clean_string)#remove punctuation except for hyphens\n",
    "        clean_string = re.sub(r\"\\bthe\\b\", r\"\", clean_string) #remove the\n",
    "        clean_string = re.sub(r'\\bwe\\b', '',clean_string ) #remove the pronoun we\n",
    "        clean_string = re.sub(r'\\bit\\b', '',clean_string )#remove the pronoun it\n",
    "        clean_string = re.sub(r'\\bthey\\b', '',clean_string )#remove the pronoun they\n",
    "        clean_string = re.sub(r'\\bcopyright\\b', '',clean_string )#remove the word copyright\n",
    "        clean_string = re.sub(r'\\bhttps\\b', '',clean_string )#remove the word https\n",
    "        clean_string = re.sub(r'\\bet\\b', '',clean_string )#remove the word et\n",
    "        clean_string = re.sub(r'\\bal\\b', '',clean_string )#remove the word al\n",
    "        clean_string = re.sub(r'\\bpreprint\\b', '',clean_string )#remove the word preprint\n",
    "        clean_string = re.sub(r'\\bthis\\b', '',clean_string )#remove the word this\n",
    "        clean_string = re.sub(r'\\bthese\\b', '',clean_string )#remove the word these\n",
    "        clean_string = re.sub(r'\\balso\\b', '',clean_string )#remove the word also\n",
    "        clean_string = re.sub(r'\\bhowever\\b', '',clean_string )#remove the word however\n",
    "        clean_string = re.sub(r'\\balthough\\b', '',clean_string )#remove the word although\n",
    "        clean_string = re.sub(r'\\bamong\\b', '',clean_string )#remove the word among\n",
    "        clean_string = re.sub(r'\\bin\\b', '',clean_string )#remove the preposition in\n",
    "        clean_string = re.sub(r'\\bwithout\\b', '',clean_string )#remove the word without\n",
    "        clean_string = re.sub(r'\\bfig\\b', '',clean_string )#remove the word fig\n",
    "        clean_string = re.sub(r'\\bfigure\\b', '', clean_string)#remove the word figure\n",
    "        clean_string = re.sub(r'\\b[a-zA-Z]\\b', '', clean_string)#remove all the single letters        \n",
    "        clean_string = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', clean_string) #remove any digit that is not part of a word\n",
    "        clean_text.append(clean_string) #add the string to the list \n",
    "stemmed_text=[] # a list which will contain the stemmed bodies of all docs      \n",
    "for val in clean_text: #for the body (which was stripped out of stop words) of each paper\n",
    "    stemmed_val=\"\" #create an empty string\n",
    "    words = val.split(' ') #split body of paper on spaces\n",
    "    for word in words: #for each word in the body\n",
    "        if(word!=\"\"): #if the word is not an empty string\n",
    "            if (word==\"coronaviruses\"):#since the stem and lemmatize functions do not recognize the word coronaviruses as the plural of coronavirus, a conidition was created\n",
    "                word =\"coronavirus\"\n",
    "            stem_lemma =wnl.lemmatize(word) if wnl.lemmatize(word).endswith('e') else porter.stem(word)\n",
    "            stemmed_val+=stem_lemma+\" \" #stem/lemmatize the word and concat it to the string stemmed_val\n",
    "    stemmed_text.append(stemmed_val) #add the string stemmed_val to the list which will contain the stemmed bodies of all docs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is no longer needed and is kept for reference\n",
    "\n",
    "\n",
    "text = ' '.join(stemmed_text) #convert all the stemmed bodies into one string containing all the stemmed bodies of all papers \n",
    "s=text.split(' ')#split the combination of papers body on space\n",
    "vocab_count = Counter(s)# count the number of vocab (unique words)\n",
    "most_occur = vocab_count.most_common(200) #get the most common 200 words\n",
    "#print(most_occur) #print the most common 200 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.03576091 ... 0.03576091 0.03576091 0.03576091]\n",
      " [0.01820378 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.03520662 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.03213882 0.         ... 0.         0.         0.        ]\n",
      " [0.01433355 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#create a tf-idf matrix for the words above\n",
    "v = TfidfVectorizer(sublinear_tf = True, min_df = 0.05,max_df = 0.8)\n",
    "#For the above vectorizer, the first paramater specifies that the 1+log(tf) is going to be used instead of tf.\n",
    "#The second parameter specifies that if a term appears in less than 5% of the docs, ignore it\n",
    "#The third parameter, considers word that do not occur in more than 80% of the corpus\n",
    "tfidf = v.fit_transform(stemmed_text) #fit and transform the stemmed texts which were previously cleaned.\n",
    "#print(sorted(v.vocabulary_.items(), key=lambda x : x[1]))\n",
    "#print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result of first common 50 words in all the 1943 docs\n",
    "#[('preprint', 42993), ('q', 29585), ('license', 20372), ('1', 18167), ('holder', 18100), ('medrxiv', 17564), ('author/funder', 17404), ('cases', 17021), ('peer-reviewed', 16697), ('https', 16223), ('data', 16221), ('covid-19', 15705), ('2', 14649), ('patients', 14258), ('doi', 14057), ('number', 13892), ('available', 13229), ('using', 11934), ('model', 11910), ('figure', 11428), ('display', 11002), ('granted', 10804), ('40', 10803), ('perpetuity', 10751), ('made', 10742), ('also', 10542), ('international', 10212), ('time', 9852), ('fig', 9747), ('3', 9574), ('used', 9344), ('cells', 9104), ('infection', 8833), ('study', 8712), ('sars-cov-2', 8633), ('r', 8195), ('5', 8153), ('this', 8143), ('virus', 8097), ('two', 7845), ('disease', 7820), ('may', 7774), ('s', 7772), ('rate', 7707), ('all', 7528), ('without', 7464), ('biorxiv', 7352), ('one', 7332), ('infected', 7203), ('10', 7201)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf- idf\n",
    "# # finding the tf-idf matrix for all the abstracts\n",
    "# # vec = TfidfVectorizer()\n",
    "\n",
    "# # ve = vec.fit_transform(clean_abs)\n",
    "\n",
    "# # # displaying the tf-idf of the word in the abstact  \n",
    "# # pd.DataFrame(ve.toarray(), columns=sorted(vec.vocabulary_.keys()))\n",
    "\n",
    "# # #applying tf-idf cosine similarity and printing the result(testing)\n",
    "# # query = [\"prophylaxis\"]\n",
    "# # query_tfidf = vec.transform(query)\n",
    "# # cosineSimilarities = cosine_similarity(query_tfidf, ve).flatten()\n",
    "# # print(cosineSimilarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taged_abs=[]\n",
    "# nouns=[]\n",
    "# # tagging the clean data\n",
    "# for val in clean_abs:\n",
    "#     taged_abs.append(pos_tag(val.split()))\n",
    "# # extracting the nouns\n",
    "# for i in range(len(taged_abs)):\n",
    "#     doc=[]\n",
    "#     for j in range(len(taged_abs[i])):\n",
    "#         if(taged_abs[i][j][1]=='NN' or taged_abs[i][j][1]=='NNS'):\n",
    "#             doc.append(taged_abs[i][j][0])\n",
    "#     nouns.append(doc)\n",
    "        \n",
    "# print(nouns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we dcided to calculate the tf-idf so that we can represent every word that is present in the abstract quatitavily. By doing so we can further use the results in order model the topic according to the abstract that we just quatified.Furthermore we will use (Non-negative Matrix Factorization) NMF in order to come up with topic's that carry most weight in the abstract. To accomplish this we are going to filter all the nouns that are avaliable in the abstract and use them to represent the different topis that are avaliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
