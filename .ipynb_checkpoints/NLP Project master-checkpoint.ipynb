{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> NLP Project </h1>\n",
    "<h2 style =\"color:blue\"> Search Engine and Literature Clustering</h2>\n",
    "<p>This notebook utilizes two major topics in natural language processing to tackle and answer questions related to COVID-19. The first one is: a <i>search engine</i> which allows the user to enter a query and fetches for them the most relevant papers related to that query (max 5). The other topic is <i>literature clustering</i> where PCA algorithm was used to reduce the dimension of the data, followed by K-means which was used to cluster the papers.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# basic libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re #import the regular expression library\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# sklearn libraries used for tfidf and cosine similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "# nltk libraries for stop word and puncuation removal.\n",
    "# nltk libraries for word lemmitization\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords #import the stopwords from the ntlk.corpus library\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize #import the word_tokenize method, which is used to turn sentences into words\"\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# !pip install scispacy scipy\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
    "import en_core_sci_lg  \n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>STEP 1: DATA PREPROCESSING</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Before applying the NLP techniques on the data, data must be preprocessed, cleaned, structured,etc. In the following cell, the folder(s) containing the data in the json format are accessed. Each json file has, then, three majors information extracted from it:<br/>\n",
    " <ol>\n",
    "     <li>The title of the paper</li>\n",
    "     <li>The abstract of the paper</li>\n",
    "     <li>The body/full text of the paper</li>\n",
    "</ol>\n",
    "These are then added to the list which will contain all the papers with those 3 features or attributes. The list is then converted to a dataframe which will be used later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1934/1934 [00:37<00:00, 51.12it/s]\n"
     ]
    }
   ],
   "source": [
    "FullPaper=[]#a list which will save all the papers \n",
    "directories = [\"biorxiv_medrxiv\"] #the folder name which contains the papers\n",
    "for directory in directories: #for each of the folders carrying the json format of different research papers\n",
    "    for file in tqdm(os.listdir(f\"{directory}/{directory}/pdf_json\")): #for every json file\n",
    "        file_path=f\"{directory}/{directory}/pdf_json/{file}\" #set the file path to the file_path variable \n",
    "        paper = json.load(open(file_path,\"rb\")) #load the json version of the file\n",
    "        title = paper['metadata']['title'] #set the title variable to the file's title\n",
    "        try:\n",
    "            abstract = paper['abstract']#set the abstract variable to the file's abstract\n",
    "        except:\n",
    "            abstarct=\"\"   #set the abstract variable to the an empty string if the file does not have an abstract             \n",
    "        full_text=\"\" #create an empty string which will hold the body of the paper (file)    \n",
    "        \n",
    "        for text in paper['body_text']: #for every word in the body of the file\n",
    "            full_text += (text['text'] +'\\n') #concatenate the word to the variable full_text\n",
    "        FullPaper.append([title,abstract,full_text]) #since the 3 variable: title, abstract, full_text, now have a value, a paper can be created and added to the FullPaper list\n",
    "        \n",
    "FullPaperDataframe=pd.DataFrame(FullPaper,columns=['title','abstract','full_text'])#create a dataframe which will hold all the papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Data Cleaning</h5>\n",
    "<p>In the below cell, each body text for each paper is stripped off stop words, punctuation, and is lemmatized. The <i>en_core_sci_lg</i> model was used as it works well on biomedical and scientific papers </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1259/1934 [52:01<30:00,  2.67s/it]  "
     ]
    }
   ],
   "source": [
    "full_text = FullPaperDataframe['full_text']\n",
    "\n",
    "clean_text = [] #a list which will hold the clean paper text (after data processing)\n",
    "dirty_text = []# a list which will hold the dirty paper text (before data processing)\n",
    "# Intializing parser lemmtizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Using the scispacy library to get the biomedical terms \n",
    "parser=en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "parser.max_length = 7000000\n",
    "\n",
    "# extending and customizing the stopwords list (Since we found these words in the text before adding the our_stopwords list)\n",
    "all_stopwords = stopwords.words('english')\n",
    "# 'the','we','it','they',\n",
    "our_stopwords=['copyright','https','et','al','preprint',\n",
    "               'this','these','also','however','although','among','in','medrxiv'\n",
    "                'biorxiv','license','without','fig','figure','doi']\n",
    "all_stopwords.extend(our_stopwords)\n",
    "\n",
    "pattern = \"\"\"!\"#$%&'()*+,.:;<=>?@[\\]^`{|}~\"\"\"\n",
    "\n",
    "#  removing all stop words and extracting puctuations\n",
    "for val in tqdm(full_text):\n",
    "    val = val.lower()\n",
    "    body_tokens = word_tokenize(val)\n",
    "    paper_body_without_stopwords =  [token for token in body_tokens if not token in all_stopwords and nltk.pos_tag([token.lower()])!='PRP']#a list which will hold the tokens stripped off stop words and words that aren't pronouns\n",
    "    dirty_paper_tokens = [token for token in body_tokens] #will hold the tokens as is\n",
    "    clean_string = ' '.join(paper_body_without_stopwords) #convert the list into string\n",
    "    clean_string = re.sub(rf\"[{pattern}]\", '', clean_string) #Strip punctuation from the text\n",
    "    clean_string = re.sub(r'\\b[a-zA-Z]\\b', '', clean_string) # from single letters from the text\n",
    "    clean_string = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', clean_string) #remove digits that aren't associated with a word from the text\n",
    "    clean_text.append(clean_string)\n",
    "    dirty_string=' '.join(dirty_paper_tokens)#convert the list of the dirty tokens into a string\n",
    "    dirty_text.append(dirty_string) #this list will be used later for analysis\n",
    "lemmatized_text=[]    # a list which will carry the lemmatized text\n",
    "\n",
    "# applying the parserer on the dataset\n",
    "for val in tqdm(clean_text):\n",
    "    lemmatized_string=[]\n",
    "    words=val.split(' ')\n",
    "    for word in words:\n",
    "        if(len(word)>1):\n",
    "            token = parser(word)\n",
    "            lemm_token = lemmatizer.lemmatize(token.text)\n",
    "            lemmatized_string.append(lemm_token)\n",
    "    if(len(lemmatized_string)>0):\n",
    "        lemmatized_text.append(lemmatized_string)\n",
    "        \n",
    "for i in range(len(lemmatized_text)):\n",
    "    lemmatized_text[i]=\" \".join(lemmatized_text[i])\n",
    "    \n",
    "#print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Corpus Word Count Graph Before Vs. After Data Cleaning </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To demonstrate how effective the data cleaning is, below is a bar chart comparing the corpus word count before and after the data cleaning.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1=0\n",
    "count2 =0\n",
    "\n",
    "for d in dirty_text: #get the word count in all the corpus before preprocessing\n",
    "    count1 +=len(d)\n",
    "\n",
    "for st in lemmatized_text:#get the word count in all the corpus after preprocessing\n",
    "    count2+=len(st)\n",
    "labels_x=[\"Before\", \"After\"]\n",
    "values = [count1, count2]\n",
    "print(count1)\n",
    "print(count2)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(labels_x, values)\n",
    "plt.xlabel('Corpus Text')\n",
    "plt.ylabel('Word Count')\n",
    "plt.title('Corpus Word Count Difference Before and After Cleaning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>STEP 2: SEARCH ENGINE IMPLEMENTATION</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>TF-IDF</h5>\n",
    "<p> After the data has been preprocessed, a TF-IDF matrix is to be created. This is a matrix which will hold the values of each word (bag of words of all the corpus) with respect to the document (paper).</p> \n",
    "<h6 style = \"color:green\"> PROS of using tf-idf</h6>\n",
    "<p>tf-idf matrix considers the unique descriptives words in a corpus and gives them high values. On the contrary, it gives low value to reptitive words in the corpus. Hence, it helps to achieve accurate results for a search engine. </p>\n",
    "<h6 style = \"color:red\"> CONS of using tf-idf</h6>\n",
    "<p>tf-idf matrix does not account for semantics at all, unlike word embeddings, for example.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tf-idf matrix for the words above\n",
    "v = TfidfVectorizer(min_df = 0.05,max_df = 0.8)\n",
    "#For the above tf-idf vectorizer:\n",
    "#The first parameter specifies that if a term appears in less than 5% of the docs, ignore it\n",
    "#The second parameter, considers word that do not occur in more than 80% of the corpus\n",
    "tfidf = v.fit_transform(lemmatized_text) #fit and transform the lemmatized texts which were previously cleaned.\n",
    "print(sorted(v.vocabulary_.items(), key=lambda x : x[1]))\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Cosine Similarity</h5>\n",
    "<p> Cosine similarity is then utilized to get the documents which are closest to the query. The closer the cosine similairty value is to 1, the closer the document is to the query (The more likely it is going to be output). However, for this notebook, we choose to display the top 5 papers with cosine similarity greater than or equal <i>0.1</i>. This number was chosen after several trials to fetch the most relevant papers for different queries.</p>\n",
    "<h6 style = \"color:green\"> PROS of using cosine similarity</h6>\n",
    "<p>Cosine similarity is of a good use, as it can match related documents and queries quite well compared to euclidean distance. Euclidean distance can classify documents and queries as non-similar even if the distribution of terms is significantly alike</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Next, the search engine will be implemented with the help of the tf-idf matrix created above and the cosine similarity\n",
    "flag = False # a flag to indicate whether or not a result is found\n",
    "pd.set_option('display.max_colwidth', -1)#to avoid dataframe column's data truncation\n",
    "query = input(\"Enter your query: \") #get the query from the user\n",
    "finalQuery=\"\" #create an empty string\n",
    "words = query.split(' ') #split the query on spaces\n",
    "for word in words: #for each word in the query\n",
    "    word = word.lower()\n",
    "    if(word!=\"\"): #if the word is not an empty string\n",
    "        if (word==\"coronaviruses\"):#since lemmatize functions do not recognize the word coronaviruses as the plural of coronavirus, a conidition was created\n",
    "            word =\"coronavirus\"\n",
    "        lem_query = lemmatizer.lemmatize(word)#lemmatize the word\n",
    "        finalQuery+=lem_query+\" \" #lemmatize the word and concat it to the string finalQuery\n",
    "query = [finalQuery.strip()]# remove any trailing spaces from the query\n",
    "query_tfidf = v.transform(query)\n",
    "cosineSimilarities = cosine_similarity(query_tfidf, tfidf).flatten() #get the cosine similarity list \n",
    "#IF NO RESULT\n",
    "print(cosineSimilarities)\n",
    "countzero_in2 = np.count_nonzero(cosineSimilarities) #get the number of non zero values in cosine similarity\n",
    "print(countzero_in2)\n",
    "if (countzero_in2==0):#if all cosine similarities are 0 (no results)\n",
    "    flag = True \n",
    "\n",
    "else:\n",
    "    if (countzero_in2>=5):#if there are 5 or more cosine similarities that are greater than 0\n",
    "        answer_indicies = sorted(range(len(cosineSimilarities)), key=lambda i: cosineSimilarities[i])[-5:] #get the top 5 indices with the highest cosine similarity\n",
    "    else: #else if they are less than 5, get all of them\n",
    "        answer_indicies = sorted(range(len(cosineSimilarities)), key=lambda i: cosineSimilarities[i])[-countzero_in2:] #get the top countzero_in2 indices with the highest cosine \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (flag == False):\n",
    "    values = [] # a list which will hold the cosine value of each paper outputted\n",
    "    for i in answer_indicies:\n",
    "        values.append(cosineSimilarities[i])\n",
    "    print(values)\n",
    "    print(answer_indicies)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.bar(answer_indicies, values,width=6)\n",
    "    plt.xlabel('Paper Index in Data Frame')\n",
    "    plt.ylabel('Cosine Similarity Value')\n",
    "    plt.title('Top Papers Retreived Cosine Similarity')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the below cell, the code verifies whether or not the cosine similarity is >=0.1.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop on the top 5 indicies and display the title, abstract, and fulltext of the paper that was fetched from the search\n",
    "if (flag == True):\n",
    "    print(\"Sorry, no results\")\n",
    "else:\n",
    "    counter =0\n",
    "    for index in answer_indicies:\n",
    "        if (cosineSimilarities[index]>=0.1): #get papers with cosine similarities greater than or equal 0.1\n",
    "            display(FullPaperDataframe.iloc[[index]])\n",
    "        else:\n",
    "            counter = counter+1\n",
    "    if (counter == len(cosineSimilarities)):\n",
    "        print(\"Sorry, no results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>STEP 4: LITERATURE CLUSTERING</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The second part of this notebook attempts to before K-Means clustering on the papers, such that each group of papers related together via certain keywords are grouped together.</p> \n",
    "<h6 style = \"color:green\"> PROS of using K-means</h6>\n",
    "<p> The pros of using K-means is that many packages provide its functionality and it is easy to implement.</p>\n",
    "<h6 style = \"color:red\"> CONS of using K-means</h6>\n",
    "<p> When clustering a new paper, the distance between that paper and every other paper should be computed, which is time consuming. Besides, the number of clusters which are to be chosen from the optimum cluster number, must be chosen.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Dimensionality Reduction</h5>\n",
    "<p>At this stage, we have lots of features/words in the TIF-IDF matrix. However, to allow the K-means to perform better, dimensionality reduction is needed. Therefore, the PCA algorithm is chosen, as it is known that it performs well with K-Means </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA #import PCA which will be used to reduce the dimensions of the dataset\n",
    "\n",
    "pca = PCA(n_components=0.90,random_state=30)#keep 90% of the variance, and start random centroid @ 30\n",
    "tf_idf_norm_kmeans= pca.fit_transform(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Elbow testing </h5>\n",
    "<p>In order to determine how many clusters are needed for the K-means. A graph is plotted, the point at which the elbow forms is then searched. That value should be used as the number of clusters. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans #import kmeans which will be used to cluster the data\n",
    "\n",
    "\n",
    "max_number_of_clusters = 18 #clusters can range from 2-max_number_of_clusters\n",
    "max_number_of_clusters = max_number_of_clusters+1 # since the range method is exclusive, a 1 should be added.\n",
    "Ks = range(2, max_number_of_clusters, 1) #Hold the range of clusters k (from 2-max_number_of_clusters)\n",
    "    \n",
    "sum_of_squared_distances = [] #a list which will carry the sum of squared distances at each k\n",
    "for k in tqdm(Ks):\n",
    "    sum_of_squared_distances.append(KMeans(n_clusters=k,random_state=30).fit(tf_idf_norm_kmeans).inertia_)#calculate the sum of squared distance for each k.\n",
    "        \n",
    "  #plot the elbow graph      \n",
    "f, ax = plt.subplots(1, 1)\n",
    "ax.plot(Ks, sum_of_squared_distances, marker='o')\n",
    "ax.set_xlabel('Cluster Centroid')\n",
    "ax.set_xticks(Ks)\n",
    "ax.set_xticklabels(Ks)\n",
    "ax.set_ylabel('Sum of Squared Distances')\n",
    "ax.set_title('Sum of Squared Distances VS. Clusters')\n",
    "plt.figure(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Elbow Graph Analysis </h5>\n",
    "<p>As exhibited in the graph above the elbow forms at k = 15</p>\n",
    "<h1 style =\"color:red\"> CHANGE K WHEN YOU RUN ALL THE DATASET. The above is for 1934 papers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> K-Means clustering</h5>\n",
    "<p>Now that the number of clusters is known, k-means can be performed as shown in the cell below </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = KMeans(n_clusters=15,random_state=30).fit_predict(tf_idf_norm_kmeans) #from the elbow graph we get the n_clusters\n",
    "df =FullPaperDataframe.copy(deep=True)  #create a new df which will hold the papers attribute along with the cluster number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the below cell, the cluster to which the paper belongs is added to the dataframe</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clusters'] = clusters\n",
    "#save the dataframe\n",
    "df.to_csv('clusters_df.csv')\n",
    "df.head() #display the head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">MISSING PLOTTING, TSNE, KEYWORD (THEME) CATEGORIZATION</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taged_abs=[]\n",
    "# nouns=[]\n",
    "# # tagging the clean data\n",
    "# for val in clean_abs:\n",
    "#     taged_abs.append(pos_tag(val.split()))\n",
    "# # extracting the nouns\n",
    "# for i in range(len(taged_abs)):\n",
    "#     doc=[]\n",
    "#     for j in range(len(taged_abs[i])):\n",
    "#         if(taged_abs[i][j][1]=='NN' or taged_abs[i][j][1]=='NNS'):\n",
    "#             doc.append(taged_abs[i][j][0])\n",
    "#     nouns.append(doc)\n",
    "        \n",
    "# print(nouns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we dcided to calculate the tf-idf so that we can represent every word that is present in the abstract quatitavily. By doing so we can further use the results in order model the topic according to the abstract that we just quatified.Furthermore we will use (Non-negative Matrix Factorization) NMF in order to come up with topic's that carry most weight in the abstract. To accomplish this we are going to filter all the nouns that are avaliable in the abstract and use them to represent the different topis that are avaliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
