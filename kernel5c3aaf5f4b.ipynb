{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re #import the regular expression library\nimport string\nimport os\nimport json\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.datasets import load_files\nfrom nltk import pos_tag\nfrom nltk.tokenize import RegexpTokenizer\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords #import the stopwords from the ntlk.corpus library\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize #import the word_tokenize method, which is used to turn sentences into words\"\nnltk.download('punkt')\nfrom collections import Counter\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nnltk.download('wordnet')\nimport numpy as np\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n# Download the spacy bio parser\nfrom IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\nimport en_core_sci_lg  # model downloaded in previous step","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"FullPaper=[]\ndirectories = [\"biorxiv_medrxiv\"]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n        if(dirname=='/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json' ):\n            file_path=f\"{dirname}/{filename}\"\n            paper = json.load(open(file_path,\"rb\"))\n            title = paper['metadata']['title'] \n            try:\n                abstract = paper['abstract']\n            except:\n                abstarct=\"\"                \n            full_text=\"\"     \n\n            for text in paper['body_text']:\n                full_text +=text['text'] +'\\n\\n' \n            FullPaper.append([title,abstract,full_text])\n\nFullPaperDataframe=pd.DataFrame(FullPaper,columns=['title','abstract','full_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Our aim is to address these points:\n* Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents."},{"metadata":{"trusted":true},"cell_type":"code","source":"full_text = FullPaperDataframe['full_text'].head(1)\n\nclean_text = []\n\nwnl = WordNetLemmatizer()\nporter = PorterStemmer()\nparser=en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\nall_stopwords = stopwords.words('english')\nour_stopwords=['the','we','it','they','copyright','https','et','al','preprint',\n               'this','these','also','however','although','among','in','medrxiv'\n                'biorxiv','license','without','fig','figure']\nall_stopwords.extend(our_stopwords)\n\npattern = \"\"\"!\"#$%&'()*+,.:;<=>?@[\\]^`{|}~\"\"\"\n\nfor val in tqdm(full_text):\n    body_tokens = word_tokenize(val)\n    paper_body_without_stopwords =  [token for token in body_tokens if not token in all_stopwords]\n    clean_string = ' '.join(paper_body_without_stopwords).lower() #convert the list into string\n    clean_string = re.sub(rf\"[{pattern}]\", '', clean_string)\n    clean_string = re.sub(r'\\b[a-zA-Z]\\b', '', clean_string)\n    clean_string = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', clean_string) \n    \n    clean_text.append(clean_string)\n    \nstemmed_text=[] # a list which will contain the stemmed bodies of all docs    \n\nfor val in clean_text: #for the body (which was stripped out of stop words) of each paper\n    stemmed_val=\"\" #create an empty string\n    words = val.split(' ') #split body of paper on spaces\n    for word in words: #for each word in the body\n        mytokens = parser(word)\n        print(mytokens)\n        print('diffffffffffff')\n        print(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this cell is no longer needed and is kept for reference\n\n\ntext = ' '.join(stemmed_text) #convert all the stemmed bodies into one string containing all the stemmed bodies of all papers \ns=text.split(' ')#split the combination of papers body on space\nvocab_count = Counter(s)# count the number of vocab (unique words)\nmost_occur = vocab_count.most_common(200) #get the most common 200 words\n#print(most_occur) #print the most common 200 words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a tf-idf matrix for the words above\nv = TfidfVectorizer(sublinear_tf = True, min_df = 0.05,max_df = 0.8)\n#For the above vectorizer, the first paramater specifies that the 1+log(tf) is going to be used instead of tf.\n#The second parameter specifies that if a term appears in less than 5% of the docs, ignore it\n#The third parameter, considers word that do not occur in more than 80% of the corpus\ntfidf = v.fit_transform(stemmed_text) #fit and transform the stemmed texts which were previously cleaned.\nprint(sorted(v.vocabulary_.items(), key=lambda x : x[1]))\nprint(tfidf.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next, the search engine will be implemented with the help of the tf-idf matrix created above and the cosine similarity\nquery = input(\"Enter your query: \") \nfinalQuery=\"\" #create an empty string\nwords = query[0].split(' ') #split the query on spaces\nprint (words)\nfor word in words: #for each word in the query\n    if(word!=\"\"): #if the word is not an empty string\n        if (word==\"coronaviruses\"):#since the stem and lemmatize functions do not recognize the word coronaviruses as the plural of coronavirus, a conidition was created\n            word =\"coronavirus\"\n        stem_lem_query =wnl.lemmatize(word) if wnl.lemmatize(word).endswith('e') else porter.stem(word)\n        finalQuery+=stem_lem_query+\" \" #stem/lemmatize the word and concat it to the string stemmed_val\nquery = [finalQuery]\nprint (query)\nquery_tfidf = v.transform(query)\ncosineSimilarities = cosine_similarity(query_tfidf, tfidf).flatten()\nprint(cosineSimilarities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# result of first common 50 words in all the 1943 docs\n#[('preprint', 42993), ('q', 29585), ('license', 20372), ('1', 18167), ('holder', 18100), ('medrxiv', 17564), ('author/funder', 17404), ('cases', 17021), ('peer-reviewed', 16697), ('https', 16223), ('data', 16221), ('covid-19', 15705), ('2', 14649), ('patients', 14258), ('doi', 14057), ('number', 13892), ('available', 13229), ('using', 11934), ('model', 11910), ('figure', 11428), ('display', 11002), ('granted', 10804), ('40', 10803), ('perpetuity', 10751), ('made', 10742), ('also', 10542), ('international', 10212), ('time', 9852), ('fig', 9747), ('3', 9574), ('used', 9344), ('cells', 9104), ('infection', 8833), ('study', 8712), ('sars-cov-2', 8633), ('r', 8195), ('5', 8153), ('this', 8143), ('virus', 8097), ('two', 7845), ('disease', 7820), ('may', 7774), ('s', 7772), ('rate', 7707), ('all', 7528), ('without', 7464), ('biorxiv', 7352), ('one', 7332), ('infected', 7203), ('10', 7201)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf- idf\n# # finding the tf-idf matrix for all the abstracts\n# # vec = TfidfVectorizer()\n\n# # ve = vec.fit_transform(clean_abs)\n\n# # # displaying the tf-idf of the word in the abstact  \n# # pd.DataFrame(ve.toarray(), columns=sorted(vec.vocabulary_.keys()))\n\n# # #applying tf-idf cosine similarity and printing the result(testing)\n# # query = [\"prophylaxis\"]\n# # query_tfidf = vec.transform(query)\n# # cosineSimilarities = cosine_similarity(query_tfidf, ve).flatten()\n# # print(cosineSimilarities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taged_abs=[]\n# nouns=[]\n# # tagging the clean data\n# for val in clean_abs:\n#     taged_abs.append(pos_tag(val.split()))\n# # extracting the nouns\n# for i in range(len(taged_abs)):\n#     doc=[]\n#     for j in range(len(taged_abs[i])):\n#         if(taged_abs[i][j][1]=='NN' or taged_abs[i][j][1]=='NNS'):\n#             doc.append(taged_abs[i][j][0])\n#     nouns.append(doc)\n        \n# print(nouns)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we dcided to calculate the tf-idf so that we can represent every word that is present in the abstract quatitavily. By doing so we can further use the results in order model the topic according to the abstract that we just quatified.Furthermore we will use (Non-negative Matrix Factorization) NMF in order to come up with topic's that carry most weight in the abstract. To accomplish this we are going to filter all the nouns that are avaliable in the abstract and use them to represent the different topis that are avaliable."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}