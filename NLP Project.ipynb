{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords #import the stopwords from the ntlk.corpus library\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize #import the word_tokenize method, which is used to turn sentences into words\"\n",
    "import re #import the regular expressions library\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories = [\"biorxiv_medrxiv\",\"noncomm_use_subset\",\"comm_use_subset\"]\n",
    "directories = [\"biorxiv_medrxiv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1934/1934 [00:28<00:00, 67.59it/s] \n"
     ]
    }
   ],
   "source": [
    "papers = [] #create a list which will hold all research papers. Each research paper will have the following: title, abstract, and body\n",
    "for directory in directories: #for each of the three folders carrying the json format of different research papers\n",
    "    for file in tqdm(os.listdir(f\"{directory}/{directory}/pdf_json\")): #for every json file\n",
    "        file_path=f\"{directory}/{directory}/pdf_json/{file}\" #set the file path to the file_path variable\n",
    "        json_file=json.load(open(file_path,\"rb\")) #read the json file\n",
    "        paper_title = json_file ['metadata']['title'] #get the paper title from that research paper\n",
    "        try:#check if the paper has an abstract\n",
    "            paper_abstract = json_file[\"abstract\"][0].values() \n",
    "        except:#if the paper does not have an abstract \n",
    "            paper_abstract=\"\" #the paper's abtract is set to an empty string\n",
    "        paper_body=\"\" #set the paper's body to an empty string initially\n",
    "        for text in json_file[\"body_text\"]:#for every text in the paper\n",
    "            paper_body += text['text']+'\\n\\n'   #concatenate it to the variable which will hold the body of the paper \n",
    "        papers.append([paper_title,paper_abstract,paper_body]) #add the paper to the papers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_abstract</th>\n",
       "      <th>paper_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The RNA pseudoknots in foot-and-mouth disease ...</td>\n",
       "      <td>(word count: 194 22 Text word count: 5168 23 2...</td>\n",
       "      <td>VP3, and VP0 (which is further processed to VP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Analysis Title: Regaining perspective on SARS-...</td>\n",
       "      <td>(During the past three months, a new coronavir...</td>\n",
       "      <td>In December 2019, a novel coronavirus, SARS-Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Healthcare-resource-adjusted vulnerabilities t...</td>\n",
       "      <td></td>\n",
       "      <td>The 2019-nCoV epidemic has spread across China...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Relationship between Average Daily Temperature...</td>\n",
       "      <td>(The rapid outbreak of the new Coronavirus pan...</td>\n",
       "      <td>The outbreak of infectious diseases has always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CHEER: hierarCHical taxonomic classification f...</td>\n",
       "      <td>(The fast accumulation of viral metagenomic da...</td>\n",
       "      <td>Metagenomic sequencing, which allows us to dir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         paper_title  \\\n",
       "0  The RNA pseudoknots in foot-and-mouth disease ...   \n",
       "1  Analysis Title: Regaining perspective on SARS-...   \n",
       "2  Healthcare-resource-adjusted vulnerabilities t...   \n",
       "3  Relationship between Average Daily Temperature...   \n",
       "4  CHEER: hierarCHical taxonomic classification f...   \n",
       "\n",
       "                                      paper_abstract  \\\n",
       "0  (word count: 194 22 Text word count: 5168 23 2...   \n",
       "1  (During the past three months, a new coronavir...   \n",
       "2                                                      \n",
       "3  (The rapid outbreak of the new Coronavirus pan...   \n",
       "4  (The fast accumulation of viral metagenomic da...   \n",
       "\n",
       "                                          paper_body  \n",
       "0  VP3, and VP0 (which is further processed to VP...  \n",
       "1  In December 2019, a novel coronavirus, SARS-Co...  \n",
       "2  The 2019-nCoV epidemic has spread across China...  \n",
       "3  The outbreak of infectious diseases has always...  \n",
       "4  Metagenomic sequencing, which allows us to dir...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(papers, columns = ['paper_title','paper_abstract','paper_body']) #created a new data frame which holds the three major cols for each paper\n",
    "df.head()#check the first 5 papers in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:13<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 666), ('.', 652), (')', 170), ('(', 166), ('The', 114), ('reads', 89), ('[', 74), (']', 74), ('preprint', 68), (':', 66), ('classification', 57), ('In', 55), ('data', 55), ('average', 54), ('species', 49), ('model', 48), ('cases', 47), ('daily', 47), ('different', 43), ('using', 43), ('rate', 42), ('temperature', 41), ('confirmed', 39), ('two', 37), ('number', 37), ('analysis', 36), ('1', 35), ('Fig', 34), ('one', 33), ('new', 33), ('license', 32), ('layer', 32), ('viral', 30), ('virus', 29), ('peer-reviewed', 29), ('available', 29), ('CHEER', 29), ('copyright', 28), ('holder', 28), ('level', 28), ('https', 27), ('epidemic', 27), ('phylogenetic', 27), ('medRxiv', 27), ('classifier', 27), ('author/funder', 26), ('3', 26), ('RNA', 26), ('tree', 26), ('training', 26)]\n"
     ]
    }
   ],
   "source": [
    "#Step 1: tokenize\n",
    "#Step 2: remove stopwords\n",
    "#Step 3: stem \"\"\n",
    "\n",
    "#Therefore, to start with step 1, fetch the paper_body col and save it in a list\n",
    "clean_text_bodies = [] #a list that will hold the bodies of all papers without stop words\n",
    "papers_body = df['paper_body'].head()\n",
    "for paperBody in tqdm(papers_body):\n",
    "        body_tokens = word_tokenize(paperBody) #tokenize the words in the body of the paper\n",
    "        paper_body_without_stopwords = [token for token in body_tokens if not token in stopwords.words('english')] #remove the stop words in the body and return a list\n",
    "        clean_string = ' '.join(paper_body_without_stopwords) #convert the list into string\n",
    "        clean_text_bodies.append(clean_string) #add the string to the array\n",
    "#Now that all the bodies have been tokenized and stop words removed. A stemming process will be applied \n",
    "stemmed_text=[]   # a list that will hold all the stemmed text    \n",
    "ps= PorterStemmer() \n",
    "for val in clean_text_bodies: #for every paper body which has been cleaned from stop words, stem it\n",
    "    stemned_val=\"\" # a string that will hold the new body of the paper after stemming\n",
    "    for word in val: #loop on every word in the clean paper body\n",
    "        stemned_val+=ps.stem(word)+\" \" #stem the word\n",
    "    stemmed_text.append(stemned_val) #added it to the list which will hold all the stemmed values\n",
    "text = ' '.join(stemmed_text)    \n",
    "s=text.split(' ')\n",
    "r = Counter(s)\n",
    "most_occur = r.most_common(50) \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incubation_df = df[df['full_text'].str.contains('incubation')]\n",
    "# # print (incubation_df.head())\n",
    "# texts = incubation_df['full_text'].values \n",
    "# for t in texts:\n",
    "#    # print (t)\n",
    "#     for sentence in t.split(\". \"):\n",
    "#         if \"antiviral\" in sentence:\n",
    "# #             arr_matches = re.findall(r\" \\d{1,2} day\",sentence) #look for regex in sentence\n",
    "#             arr_matches = re.findall(r\"\\w*-19\\b\",sentence)\n",
    "#             if (len(arr_matches) ==1):\n",
    "#                 print(arr_matches[0])\n",
    "#                 print(sentence) \n",
    "#                 print()\n",
    "#                 print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         for key in j:\n",
    "#             print(key)\n",
    "# print (j['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in j['metadata']:\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
